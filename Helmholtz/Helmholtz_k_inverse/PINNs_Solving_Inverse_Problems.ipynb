{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "### Poisson equation (2D)\n",
    "\n",
    "$$-\\Delta u =f(x,y)$$\n",
    "\n",
    " \n",
    "\n",
    "### Problem\n",
    "\n",
    "$$-\\Delta u=2\\pi^2sin(\\pi x)sin(\\pi y)$$\n",
    "\n",
    "### Boundary Conditions:\n",
    "\n",
    "$$u(-1,y)=0,\\  u(1,y) =0, \\ u(x,-1)=0,\\  u(x,1) =0$$\n",
    "\n",
    "### Exact solution:\n",
    "\n",
    "$$u(x)=sin(\\pi x)sin(\\pi y)$$\n",
    "\n",
    "\n",
    "So the residual will be:\n",
    "\n",
    "$$0=\\frac{\\partial^2u}{\\partial x^2}+\\frac{\\partial^2u}{\\partial y^2}+2\\pi^2sin(\\pi x)sin(\\pi y)$$\n",
    "\n",
    "Note: Remeber that our neural network $NN(x,y)\\approx u(x,y)$ so:\n",
    "\n",
    "$$\\frac{\\partial^2NN}{\\partial x^2}\\approx\\frac{\\partial^2u}{\\partial x^2}$$\n",
    "$$\\frac{\\partial^2NN}{\\partial y^2}\\approx\\frac{\\partial^2u}{\\partial y^2}$$\n",
    "\n",
    "$$f=\\frac{\\partial^2NN}{\\partial x^2}+\\frac{\\partial^2NN}{\\partial y^2}+2\\pi^2sin(\\pi x)sin(\\pi y)\\rightarrow 0$$\n",
    "\n",
    "\n",
    "**Initial Conditions (Dirichlet BC)**\n",
    "$$u(-1,y)=0,\\ u(1,y)=0$$\n",
    "\n",
    "$$u(x,-1)=0,\\ u(x,1)=0$$\n",
    "\n",
    "Set two functions to describe our boundary conditions:\n",
    "\n",
    "$$f_{BC_v}(x)=1-|x|,\\  x = \\pm  1 \\ y \\in (-1,1)$$\n",
    "$$f_{BC_h}(x)=1-|y|,\\  y = \\pm 1 \\ x \\in (-1,1)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name()) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps=5000\n",
    "lr=1e-3\n",
    "layers = np.array([2,50,50,20,50,50,1]) #5 hidden layers\n",
    "# To generate new data:\n",
    "x_l=-1\n",
    "x_r=1\n",
    "y_l=-1\n",
    "y_r=1\n",
    "total_points=800\n",
    "#Nu: Number of training points (2 as we onlt have 2 boundaries), # Nf: Number of collocation points (Evaluate PDE)\n",
    "Nu=1000\n",
    "Nf=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_BCv(x):\n",
    "    return 1-torch.abs(x)\n",
    "\n",
    "def f_BCh(y):\n",
    "    return 1-torch.abs(y)\n",
    "\n",
    "def f_real(x,y):\n",
    "    return torch.sin(np.pi*x)*torch.sin(np.pi*y)\n",
    "\n",
    "def PDE(x,y):\n",
    "    return -2*(np.pi**2)*torch.sin(np.pi*x)*torch.sin(np.pi*y)\n",
    "\n",
    "def plot3D(X,Y,f,title):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot_surface(X.numpy(), Y.numpy(), f.numpy(), cmap='jet', edgecolor='none')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('f(X, Y)')\n",
    "    ax.set_title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plot_heat(x_l,x_r,y_l,y_r,f,title_colorbar):\n",
    "    plt.imshow(f.numpy(), cmap='jet', origin='lower', extent=[x_l, x_r, y_l, y_r])\n",
    "    plt.colorbar(label='f(x, y)')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title(title_colorbar)\n",
    "\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(nn.Module):\n",
    "    ##Neural Network\n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)]) \n",
    "        self.iter = 0\n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "\n",
    "            \n",
    "            \n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "    'foward pass'\n",
    "    def forward(self,X):\n",
    "        if torch.is_tensor(X) != True:         \n",
    "            X = torch.from_numpy(X)                \n",
    "        a = X.float()\n",
    "        for i in range(len(layers)-2):  \n",
    "            z = self.linears[i](a)              \n",
    "            a = self.activation(z)    \n",
    "        a = self.linears[-1](a)\n",
    "        return a\n",
    "    'Loss Functions'\n",
    "    #Loss BC\n",
    "    def lossBC(self,X_BC,f_BC):\n",
    "        loss_BC=self.loss_function(self.forward(X_BC),f_BC)\n",
    "        return loss_BC\n",
    "    #Loss PDE\n",
    "    def lossPDE(self,X_PDE):\n",
    "        g=X_PDE.clone()\n",
    "        g.requires_grad=True #Enable differentiation\n",
    "        f=self.forward(g)\n",
    "        f_X=autograd.grad(f,g,grad_outputs=torch.ones_like(f),create_graph=True)[0] #first derivative    \n",
    "        grad_x =   f_X[:,0]\n",
    "        grad_y = f_X[:,1]\n",
    "        f_XX = autograd.grad(grad_x, g, grad_outputs=torch.ones_like(grad_x), create_graph=True)[0][:, 0]\n",
    "        f_YY = autograd.grad(grad_y, g, grad_outputs=torch.ones_like(grad_y), create_graph=True)[0][:, 1]\n",
    "        \n",
    "        solu = 2*np.pi ** 2 *torch.sin(np.pi * g[:, 0:1])*torch.sin(np.pi * g[:, 1:])\n",
    "        delta_f=f_XX.unsqueeze(1) + f_YY.unsqueeze(1) + solu\n",
    "        \n",
    "         \n",
    "        \n",
    "        \n",
    "        return self.loss_function(delta_f,f_hat)\n",
    "#     + 2*np.pi ** 2 *torch.sin(np.pi * g[:, 0:1])*torch.sin(np.pi * g[:, 1:])\n",
    "        \n",
    "    def loss(self,X_BC,f_BC,X_PDE):\n",
    "        loss_bc=self.lossBC(X_BC,f_BC)\n",
    "        loss_pde=self.lossPDE(X_PDE)\n",
    "        return loss_bc+loss_pde\n",
    "#X_train_Nu,v_train_Nu,X_train_Nf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the function as a 3D heatmap\n",
    "x = torch.linspace(x_l,x_r,total_points).view(-1,1) #prepare to NN\n",
    "y = torch.linspace(y_l,y_r,total_points).view(-1,1) #prepare to NN\n",
    "\n",
    "X, Y = torch.meshgrid(x.view(-1,), y.view(-1,))\n",
    "f = f_real(X,Y)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the mesh into a 2-column vector\n",
    "x_test=torch.hstack((X.transpose(1,0).flatten()[:,None],Y.transpose(1,0).flatten()[:,None]))\n",
    "\n",
    "v_test=f.transpose(1,0).flatten()[:,None] # Colum major Flatten (so we transpose it)\n",
    " \n",
    "# Domain bounds\n",
    "lb=x_test[0] #first value\n",
    "ub=x_test[-1] #last value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Boundary Conditions\n",
    "#Left Edge \n",
    "left_X=torch.hstack((X[0,:][:,None],Y[0,:][:,None])) # First column # The [:,None] is to give it the right dimension\n",
    "vleft_boundary = f_BCv(left_X[:,0]).unsqueeze(1)\n",
    "\n",
    "\n",
    "\n",
    "#right Edge\n",
    "right_X=torch.hstack((X[-1,:][:,None],Y[-1,:][:,None]))\n",
    "vright_boundary = f_BCv(right_X[:,0]).unsqueeze(1)\n",
    " \n",
    "\n",
    "#Bottom Edge\n",
    "bottom_X=torch.hstack((X[:,0][:,None],Y[:,0][:,None])) # First row # The [:,None] is to give it the right dimension\n",
    "vbottom_boundary = f_BCh(bottom_X[:,-1]) .unsqueeze(1)\n",
    " \n",
    "\n",
    "#Top Edge\n",
    "top_X=torch.hstack((X[:,0][:,None],Y[:,-1][:,None])) # First row # The [:,None] is to give it the right dimension\n",
    "vtop_boundary = f_BCh(top_X[:,-1]).unsqueeze(1)\n",
    " \n",
    "# #Get all the training data into the same dataset\n",
    "X_train=torch.vstack([left_X,right_X,bottom_X,top_X])\n",
    "v_train=torch.vstack([vleft_boundary,vright_boundary,vbottom_boundary,vtop_boundary])\n",
    "#Choose(Nu) points of our available training data:\n",
    "idx=np.random.choice(X_train.shape[0],Nu,replace=False)\n",
    "X_train_Nu=X_train[idx,:]\n",
    "v_train_Nu=v_train[idx,:]\n",
    "  \n",
    "# Collocation Points (Evaluate our PDe)\n",
    "#Choose(Nf) points(Latin hypercube)\n",
    "X_train_Nf=lb+(ub-lb)*lhs(2,Nf) # 2 as the inputs are x and y\n",
    "X_train_Nf=torch.vstack((X_train_Nf,X_train_Nu)) #Add the training poinst to the collocation points\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(123)\n",
    "#Store tensors to GPU\n",
    "X_train_Nu=X_train_Nu.float().to(device)#Training Points (BC)\n",
    "v_train_Nu=v_train_Nu.float().to(device)#Training Points (BC)\n",
    "X_train_Nf=X_train_Nf.float().to(device)#Collocation Points\n",
    "f_hat = torch.zeros(X_train_Nf.shape[0],1).to(device)#to minimize function\n",
    "print(f_hat.shape)\n",
    "\n",
    "X_test=x_test.float().to(device) # the input dataset (complete)\n",
    "V_test=v_test.float().to(device) # the real solution \n",
    "\n",
    "\n",
    "model = PINN(layers)\n",
    "print(model)\n",
    "model.to(device)\n",
    "params = list(model.parameters())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, amsgrad=False)\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.lossBC(X_train_Nu,v_train_Nu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(steps):\n",
    "     \n",
    "    loss = model.loss(X_train_Nu,v_train_Nu,X_train_Nf)# use mean squared error\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i%200==0:\n",
    "        print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model(x_test).detach()\n",
    " \n",
    "reshaped_f = torch.reshape(y_predict, X.shape).transpose(1,0)\n",
    "\n",
    "title = 'Plot of f(x,y) = sin(pi*x)*sin(pi*y)'\n",
    "plot3D(X,Y,f,title)\n",
    "title_colorbar = 'Color plot of f(x,y) = sin(pi*x)*sin(pi*y)'\n",
    "plot_heat(x_l,x_r,y_l,y_r,f,title_colorbar)\n",
    "\n",
    "title = 'Plot of predicted solution'\n",
    "plot3D(X,Y,reshaped_f,title)\n",
    "title_colorbar = 'Color plot of predicted solution'\n",
    "plot_heat(x_l,x_r,y_l,y_r,reshaped_f,title_colorbar)\n",
    "\n",
    "title_colorbar = 'Color plot of absolute error' \n",
    "plot_heat(x_l,x_r,y_l,y_r,reshaped_f-f,title_colorbar) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data-driven discovery of partial differential equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous time models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, the authors define the neural network that will be used to solve the inverse problems of partial differential equations which have parameters in the PDE:\n",
    "$$u_t + \\mathcal{N}(u,\\lambda) = 0$$\n",
    "Similar to the above Data-driven solutions of partial differential equations, we can define the neural network as\n",
    "$$f(x,t) := u_t + \\mathcal{N}(u,\\lambda)$$\n",
    "The parameters of the differential operator $\\lambda$ turn into parameters of the physics-informed neural network.To illustrate the application of Physics-Informed Neural Networks (PINN) for solving partial differential equations, we provide the code implementation for the Poisson equation below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup\n",
    "\n",
    "### Burgers equation\n",
    "\n",
    "$$\\frac{\\partial u}{\\partial t}  + \\lambda_1 u\\frac{\\partial u}{\\partial x} - \\lambda_2 \\frac{\\partial^2 u}{\\partial x^2} =0$$\n",
    "By using training data selected from the initial, boundary, and inner domain data, we can determine the parameters $\\lambda_1$ and $\\lambda_2$ in the Burgers equation. It is important to emphasize that our objective in this problem is to solve for the parameters in the Burgers equation, rather than obtaining a numerical solution of the equation itself. The training data can be chosen from the inner domain, and there is no need to identify specific initial and boundary training data or collocation points. To tackle this problem, we can treat the parameters of the Burgers equation as the parameters of the neural network, denoted as $f(\\theta,x,t) := \\frac{\\partial u_{NN}(\\theta,x,t)}{\\partial t} + \\lambda_1 u_{NN}(\\theta,x,t) \\frac{\\partial u_{NN}(\\theta,x,t)}{\\partial x} - \\lambda_2 \\frac{\\partial^2 u_{NN}(\\theta,x,t)}{\\partial x^2}$, where $\\theta$ is the weight of the neural network. The loss function is the same as above:\n",
    "$$MSE = MSE_u + MSE_f$$\n",
    "where\n",
    "$$MSE_u = \\frac{1}{N_u}\\sum_{i=1}^{N_u}|u(t^{i}_u,x_u^i)-u^{i}|^2$$\n",
    "and \n",
    "$$MSE_f = \\frac{1}{N_f}\\sum_{i=1}^{N_f}|f(t^{i}_f,x_f^i)|^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "from collections import OrderedDict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "\n",
    "import scipy.io\n",
    "from scipy.interpolate import griddata\n",
    "import sys\n",
    "import os\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:128\"\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "np.random.seed(1235)\n",
    "\n",
    "\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu = 0.01/np.pi\n",
    "N_u = 2000\n",
    "#layers = [2, 20, 20, 20 ,1]\n",
    "layers = [2, 50, 50, 50,  1]\n",
    "\n",
    "data = scipy.io.loadmat('data/burgers_shock.mat')\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'].flatten()[:,None]\n",
    "Exact = np.real(data['usol']).T\n",
    "X, T = np.meshgrid(x,t)\n",
    "X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "u_star = Exact.flatten()[:,None]     \n",
    "\n",
    "# k_true =2#波数（单频）\n",
    "# a1=1\n",
    "# a2=4\n",
    "# lin_num = 51\n",
    "# t = np.linspace(3,5,lin_num)[:,None] #Y采样点\n",
    "# x = np.linspace(3,5,lin_num)[:,None] #X采样点\n",
    "# X, T = np.meshgrid(x,t)\n",
    "# Exact = np.sin(k_true*X) * np.sin(k_true*T) #网格化坐标上的真解\n",
    "# X_star = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "# u_star = (0.5*np.sin(k_true*X) * np.sin(k_true* T)).flatten()[:,None]#网格化坐标上的真解      \n",
    "\n",
    "\n",
    "# Doman bounds\n",
    "lb = X_star.min(0)\n",
    "ub = X_star.max(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Physics-informed Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the deep neural network\n",
    "class Solu_NN(torch.nn.Module):\n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "    \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            \n",
    "            # weights from a normal distribution with Recommended gain value?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            \n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,X):\n",
    "         \n",
    "        a = X.float()\n",
    "\n",
    "\n",
    "        for i in range(len(layers)-2):\n",
    "\n",
    "            z = self.linears[i](a)\n",
    "\n",
    "            a = self.activation(z)\n",
    "\n",
    "        out = self.linears[-1](a)\n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4064861893.py, line 98)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[8], line 98\u001b[1;36m\u001b[0m\n\u001b[1;33m    self.loss_pde = []\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# the physics-guided neural network\n",
    "class PINN():\n",
    "    def __init__(self, X, u, layers, lb, ub):\n",
    "        \n",
    "        # boundary conditions\n",
    "        self.lb = torch.tensor(lb).float().to(device)\n",
    "        self.ub = torch.tensor(ub).float().to(device)\n",
    "        \n",
    "        # data\n",
    "        self.x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        self.t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "        self.u = torch.tensor(u).float().to(device)\n",
    "        \n",
    "        # settings\n",
    "        self.lambda_1 = torch.tensor([0.0], requires_grad=True).to(device)\n",
    "        self.lambda_2 = torch.tensor([-6.0], requires_grad=True).to(device)\n",
    "        self.k = torch.tensor([.1], requires_grad=True).to(device)  # 初始化一个波数（自己选择）  \n",
    "\n",
    "        self.lambda_1 = torch.nn.Parameter(self.lambda_1)\n",
    "        self.lambda_2 = torch.nn.Parameter(self.lambda_2)\n",
    "        self.k = torch.nn.Parameter(self.k)\n",
    "        # deep neural networks\n",
    "        self.u_nn = Solu_NN(layers).to(device)\n",
    "        self.u_nn.register_parameter('lambda_1', self.lambda_1)\n",
    "        self.u_nn.register_parameter('lambda_2', self.lambda_2)\n",
    "        self.u_nn.register_parameter('k', self.k)\n",
    "         # optimizers: using the same settings\n",
    "        self.optimizer = torch.optim.LBFGS(\n",
    "            self.u_nn.parameters(), \n",
    "            lr=1.0, \n",
    "            max_iter=50000, \n",
    "            max_eval=50000, \n",
    "            history_size=50,\n",
    "            tolerance_grad=1e-5, \n",
    "            tolerance_change=1.0 * np.finfo(float).eps,\n",
    "            line_search_fn=\"strong_wolfe\"       # can be \"strong_wolfe\"\n",
    "        )\n",
    "        \n",
    "        #记录损失\n",
    "        self.loss_pde = []\n",
    "        self.loss_data = []\n",
    "        self.loss_total = []\n",
    "        self.iter_list = []\n",
    "        self.iter = 0\n",
    "        \n",
    "    def net_u(self, x, t):  \n",
    "        u = self.u_nn(torch.cat([x, t], dim=1))\n",
    "        return u\n",
    "    \n",
    "    def net_f(self, x, t):\n",
    "        \"\"\" The pytorch autograd version of calculating residual \"\"\"\n",
    "        lambda_1 = self.lambda_1        \n",
    "        lambda_2 = torch.exp(self.lambda_2)\n",
    "        #lambda_2 = self.lambda_2\n",
    "        k =self.k\n",
    "        #k =torch.exp(self.k)\n",
    "        u = self.net_u(x, t)\n",
    "        \n",
    "        u_t = torch.autograd.grad(u, t, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        u_tt = torch.autograd.grad(u_t, t, grad_outputs=torch.ones_like(u_t), retain_graph=True, create_graph=True)[0]\n",
    "        u_x = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), retain_graph=True, create_graph=True)[0]\n",
    "        u_xx = torch.autograd.grad(u_x, x, grad_outputs=torch.ones_like(u_x), retain_graph=True, create_graph=True)[0]\n",
    "\n",
    "        #f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\n",
    "        #q = (-(a1*np.pi)**2-(a2*np.pi)**2+k**2)*np.sin(a1*np.pi*self.x)*np.sin(a2*np.pi*self.t)\n",
    "        f = u_tt + u_xx +k**2*u\n",
    "\n",
    "        loss_g_x = torch.autograd.grad(f, x, grad_outputs=torch.ones_like(f), retain_graph=True, create_graph=True)[0]\n",
    "        loss_g_t = torch.autograd.grad(f, t, grad_outputs=torch.ones_like(f), retain_graph=True, create_graph=True)[0]\n",
    "        return f,loss_g_x,loss_g_t\n",
    "    \n",
    "    def loss_func(self):\n",
    "        u_pred = self.net_u(self.x, self.t)\n",
    "        f_pred,loss_g_x,loss_g_t = self.net_f(self.x, self.t)\n",
    "        loss_data = torch.mean((self.u - u_pred)**2)\n",
    "        loss_pde = torch.mean(f_pred**2 )\n",
    "        loss_gradient =torch.mean(loss_g_x**2)+torch.mean(loss_g_t**2)\n",
    "        loss = loss_data+loss_pde#+loss_gradient\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "        if self.iter % 100 == 0:\n",
    "            (\n",
    "            print('Loss: %e, loss_data: %e, loss_pde: %e,  loss_gra: %e, lambda0_1: %.5f, lambda0_2: %.5f, k: %.5f' )\n",
    "                \n",
    "\n",
    "                    loss.item(), \n",
    "                    loss_data.item(), \n",
    "                    loss_pde.item(), \n",
    "                    loss_gradient.item(),\n",
    "                    self.lambda_1.item(), \n",
    "                    torch.exp(self.lambda_2.detach()).item(),\n",
    "                    #self.lambda_2.item(),\n",
    "                    self.k.item()\n",
    "                    #torch.exp(self.k.detach()).item(),\n",
    "                    #形成曲线\n",
    "\n",
    "            \n",
    "            )\n",
    "             \n",
    "        return loss\n",
    "    \n",
    "    def train(self, nIter):\n",
    "        self.u_nn.train()\n",
    "        # Backward and optimize\n",
    "\n",
    "        self.loss_pde = []\n",
    "        self.loss_data = []\n",
    "        self.loss_total = []\n",
    "        self.iter_list = []\n",
    "\n",
    "        self.optimizer.step(self.loss_func)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        x = torch.tensor(X[:, 0:1], requires_grad=True).float().to(device)\n",
    "        t = torch.tensor(X[:, 1:2], requires_grad=True).float().to(device)\n",
    "\n",
    "        self.u_nn.eval()\n",
    "        u = self.net_u(x, t)\n",
    "        f = self.net_f(x, t)\n",
    "        u = u.detach().cpu().numpy()\n",
    "        f = f.detach().cpu().numpy()\n",
    "        return u, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Non-noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PINN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:13\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PINN' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "noise = 0.0            \n",
    "N_u = 5000\n",
    "# create training set\n",
    "np.random.seed(2235)\n",
    "# idx = np.random.choice(X_star.shape[0], N_u, replace=False)\n",
    "# X_u_train = X_star[idx,:]\n",
    "# u_train = u_star[idx,:]\n",
    "\n",
    "X_u_train = X_star\n",
    "u_train = u_star\n",
    "\n",
    "# training\n",
    "model = PINN(X_u_train, u_train, layers, lb, ub)\n",
    "loss_data_init = torch.mean((model.u - model.net_u(model.x,model.t)) ** 2)\n",
    "loss_pde_init = torch.mean(model.net_f(model.x,model.t)[0] ** 2)\n",
    "loss_gra_x_init = torch.mean(model.net_f(model.x,model.t)[1] ** 2)\n",
    "loss_gra_y_init = torch.mean(model.net_f(model.x,model.t)[2] ** 2)\n",
    "print('loss_data_init: %e'%loss_data_init.item())\n",
    "print('loss_pde_init: %e'%loss_pde_init.item())\n",
    "print('loss_gra_x_init: %e'%loss_gra_x_init.item())\n",
    "print('loss_gra_y_init: %e'%loss_gra_y_init.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.565561e-02, loss_data: 2.466564e-02, loss_pde: 9.899720e-04,  loss_gra: 1.593132e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.55904\n",
      "Loss: 2.420829e-02, loss_data: 2.268021e-02, loss_pde: 1.528083e-03,  loss_gra: 2.243961e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.74923\n",
      "Loss: 2.386292e-02, loss_data: 2.243884e-02, loss_pde: 1.424073e-03,  loss_gra: 2.144759e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.77828\n",
      "Loss: 2.353103e-02, loss_data: 2.201733e-02, loss_pde: 1.513693e-03,  loss_gra: 2.299281e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.81977\n",
      "Loss: 2.325147e-02, loss_data: 2.170023e-02, loss_pde: 1.551241e-03,  loss_gra: 2.208852e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.84004\n",
      "Loss: 2.286558e-02, loss_data: 2.144413e-02, loss_pde: 1.421450e-03,  loss_gra: 2.364868e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.91659\n",
      "Loss: 2.213837e-02, loss_data: 2.106290e-02, loss_pde: 1.075466e-03,  loss_gra: 1.791439e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.92715\n",
      "Loss: 2.176755e-02, loss_data: 2.042138e-02, loss_pde: 1.346169e-03,  loss_gra: 3.342434e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.92494\n",
      "Loss: 2.119861e-02, loss_data: 1.924718e-02, loss_pde: 1.951429e-03,  loss_gra: 7.071647e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.93955\n",
      "Loss: 2.079646e-02, loss_data: 1.876417e-02, loss_pde: 2.032284e-03,  loss_gra: 6.914110e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.94050\n",
      "Loss: 2.006437e-02, loss_data: 1.842338e-02, loss_pde: 1.640992e-03,  loss_gra: 5.226363e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.96483\n",
      "Loss: 1.975682e-02, loss_data: 1.798700e-02, loss_pde: 1.769827e-03,  loss_gra: 5.117582e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 1.97448\n",
      "Loss: 1.965828e-02, loss_data: 1.773719e-02, loss_pde: 1.921095e-03,  loss_gra: 5.982381e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.00591\n",
      "Loss: 1.925044e-02, loss_data: 1.765542e-02, loss_pde: 1.595021e-03,  loss_gra: 4.834433e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.05094\n",
      "Loss: 1.899407e-02, loss_data: 1.765295e-02, loss_pde: 1.341121e-03,  loss_gra: 3.944222e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.05333\n",
      "Loss: 1.889337e-02, loss_data: 1.745468e-02, loss_pde: 1.438691e-03,  loss_gra: 4.158102e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.03796\n",
      "Loss: 1.874824e-02, loss_data: 1.715695e-02, loss_pde: 1.591284e-03,  loss_gra: 4.998489e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.04430\n",
      "Loss: 1.847466e-02, loss_data: 1.695297e-02, loss_pde: 1.521689e-03,  loss_gra: 5.522898e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.05547\n",
      "Loss: 1.833840e-02, loss_data: 1.672851e-02, loss_pde: 1.609895e-03,  loss_gra: 5.085335e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.02598\n",
      "Loss: 1.825514e-02, loss_data: 1.670868e-02, loss_pde: 1.546453e-03,  loss_gra: 5.126983e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.03976\n",
      "Loss: 1.810940e-02, loss_data: 1.650118e-02, loss_pde: 1.608224e-03,  loss_gra: 5.693857e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.06629\n",
      "Loss: 1.801975e-02, loss_data: 1.652517e-02, loss_pde: 1.494581e-03,  loss_gra: 5.040061e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.05640\n",
      "Loss: 1.790598e-02, loss_data: 1.634395e-02, loss_pde: 1.562031e-03,  loss_gra: 4.912357e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.07294\n",
      "Loss: 1.782018e-02, loss_data: 1.614473e-02, loss_pde: 1.675454e-03,  loss_gra: 5.246206e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.06929\n",
      "Loss: 1.762893e-02, loss_data: 1.581673e-02, loss_pde: 1.812202e-03,  loss_gra: 5.556371e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.05911\n",
      "Loss: 1.732364e-02, loss_data: 1.570039e-02, loss_pde: 1.623245e-03,  loss_gra: 5.427965e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.08978\n",
      "Loss: 1.724089e-02, loss_data: 1.577115e-02, loss_pde: 1.469745e-03,  loss_gra: 4.772578e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.10387\n",
      "Loss: 1.702499e-02, loss_data: 1.548343e-02, loss_pde: 1.541558e-03,  loss_gra: 5.103859e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.10247\n",
      "Loss: 1.666259e-02, loss_data: 1.548967e-02, loss_pde: 1.172922e-03,  loss_gra: 4.673196e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.13856\n",
      "Loss: 1.657364e-02, loss_data: 1.553642e-02, loss_pde: 1.037223e-03,  loss_gra: 3.885578e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.14727\n",
      "Loss: 1.650594e-02, loss_data: 1.548695e-02, loss_pde: 1.018989e-03,  loss_gra: 3.889059e-02, lambda0_1: 0.00000, lambda0_2: 0.00248, k: 2.14071\n"
     ]
    }
   ],
   "source": [
    "model.train(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1698], device='cuda:0', grad_fn=<ExpBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.exp(model.k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.67 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32me:\\acedemic_remake\\My_Version\\Helmholtz\\Helmholtz_k_inverse\\PINNs_Solving_Inverse_Problems.ipynb 单元格 34\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# evaluations\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m u_pred, f_pred \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(X_star)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m error_u \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(u_star\u001b[39m-\u001b[39mu_pred,\u001b[39m2\u001b[39m)\u001b[39m/\u001b[39mnp\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(u_star,\u001b[39m2\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m U_pred \u001b[39m=\u001b[39m griddata(X_star, u_pred\u001b[39m.\u001b[39mflatten(), (X, T), method\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcubic\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32me:\\acedemic_remake\\My_Version\\Helmholtz\\Helmholtz_k_inverse\\PINNs_Solving_Inverse_Problems.ipynb 单元格 34\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mu_nn\u001b[39m.\u001b[39meval()\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m u \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnet_u(x, t)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnet_f(x, t)\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m u \u001b[39m=\u001b[39m u\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[0;32m    <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m f \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "\u001b[1;32me:\\acedemic_remake\\My_Version\\Helmholtz\\Helmholtz_k_inverse\\PINNs_Solving_Inverse_Problems.ipynb 单元格 34\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m \u001b[39m#f = u_t + lambda_1 * u * u_x - lambda_2 * u_xx\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m \u001b[39m#q = (-(a1*np.pi)**2-(a2*np.pi)**2+k**2)*np.sin(a1*np.pi*self.x)*np.sin(a2*np.pi*self.t)\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m f \u001b[39m=\u001b[39m u_tt \u001b[39m+\u001b[39m u_xx \u001b[39m+\u001b[39mk\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m*\u001b[39mu\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m loss_g_x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mgrad(f, x, grad_outputs\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mones_like(f), retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, create_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m loss_g_t \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mgrad(f, t, grad_outputs\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mones_like(f), retain_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, create_graph\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/acedemic_remake/My_Version/Helmholtz/Helmholtz_k_inverse/PINNs_Solving_Inverse_Problems.ipynb#X45sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39mreturn\u001b[39;00m f,loss_g_x,loss_g_t\n",
      "File \u001b[1;32md:\\setup_position_1\\anaconda\\envs\\pytorch-gpu\\lib\\site-packages\\torch\\autograd\\__init__.py:303\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched)\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[39mreturn\u001b[39;00m _vmap_internals\u001b[39m.\u001b[39m_vmap(vjp, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, allow_none_pass_through\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)(grad_outputs_)\n\u001b[0;32m    302\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 303\u001b[0m     \u001b[39mreturn\u001b[39;00m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    304\u001b[0m         t_outputs, grad_outputs_, retain_graph, create_graph, t_inputs,\n\u001b[0;32m    305\u001b[0m         allow_unused, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 4.00 GiB total capacity; 2.67 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# evaluations\n",
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "error_u = np.linalg.norm(u_star-u_pred,2)/np.linalg.norm(u_star,2)\n",
    "\n",
    "U_pred = griddata(X_star, u_pred.flatten(), (X, T), method='cubic')\n",
    "\n",
    "lambda_1_value = model.lambda_1.detach().cpu().numpy()\n",
    "lambda_2_value = model.lambda_2.detach().cpu().numpy()\n",
    "#lambda_2_value = np.exp(lambda_2_value)\n",
    "\n",
    "error_lambda_1 = np.abs(lambda_1_value - 1.0) * 100\n",
    "error_lambda_2 = np.abs(lambda_2_value - nu) / nu * 100\n",
    "\n",
    "print('Error u: %e' % (error_u))    \n",
    "print('Error l1: %.5f%%' % (error_lambda_1))                             \n",
    "print('Error l2: %.5f%%' % (error_lambda_2))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = 0.01    \n",
    "\n",
    "# create training set\n",
    "u_train = u_train + noise*np.std(u_train)*np.random.randn(u_train.shape[0], u_train.shape[1])\n",
    "\n",
    "# training\n",
    "model = PINN(X_u_train, u_train, layers, lb, ub)\n",
    "model.train(0)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluations\n",
    "u_pred, f_pred = model.predict(X_star)\n",
    "\n",
    "lambda_1_value_noisy = model.lambda_1.detach().cpu().numpy()\n",
    "lambda_2_value_noisy = model.lambda_2.detach().cpu().numpy()\n",
    "lambda_2_value_noisy = np.exp(lambda_2_value_noisy)\n",
    "\n",
    "error_lambda_1_noisy = np.abs(lambda_1_value_noisy - 1.0) * 100\n",
    "error_lambda_2_noisy = np.abs(lambda_2_value_noisy - nu) / nu * 100\n",
    "\n",
    "print('Error u: %e' % (error_u))    \n",
    "print('Error l1: %.5f%%' % (error_lambda_1_noisy))                             \n",
    "print('Error l2: %.5f%%' % (error_lambda_2_noisy))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "####### Row 0: u(t,x) ##################    \n",
    "\n",
    "fig = plt.figure(figsize=(9, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "h = ax.imshow(U_pred.T, interpolation='nearest', cmap='rainbow', \n",
    "              extent=[t.min(), t.max(), x.min(), x.max()], \n",
    "              origin='lower', aspect='auto')\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.10)\n",
    "cbar = fig.colorbar(h, cax=cax)\n",
    "cbar.ax.tick_params(labelsize=15) \n",
    "\n",
    "ax.plot(\n",
    "    X_u_train[:,1], \n",
    "    X_u_train[:,0], \n",
    "    'kx', label = 'Data (%d points)' % (u_train.shape[0]), \n",
    "    markersize = 4,  # marker size doubled\n",
    "    clip_on = False,\n",
    "    alpha=.5\n",
    ")\n",
    "\n",
    "line = np.linspace(x.min(), x.max(), 2)[:,None]\n",
    "ax.plot(t[25]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[50]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "ax.plot(t[75]*np.ones((2,1)), line, 'w-', linewidth = 1)\n",
    "\n",
    "ax.set_xlabel('$t$', size=20)\n",
    "ax.set_ylabel('$x$', size=20)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.9, -0.05), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "ax.set_title('$u(t,x)$', fontsize = 20) # font size doubled\n",
    "ax.tick_params(labelsize=15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### Row 1: u(t,x) slices ################## \n",
    "\n",
    "\"\"\" The aesthetic setting has changed. \"\"\"\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "gs1 = gridspec.GridSpec(1, 3)\n",
    "gs1.update(top=1-1.0/3.0-0.1, bottom=1.0-2.0/3.0, left=0.1, right=0.9, wspace=0.5)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 0])\n",
    "ax.plot(x,Exact[25,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[25,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')    \n",
    "ax.set_title('$t = 0.25$', fontsize = 15)\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 1])\n",
    "ax.plot(x,Exact[50,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[50,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])\n",
    "ax.set_title('$t = 0.50$', fontsize = 15)\n",
    "ax.legend(\n",
    "    loc='upper center', \n",
    "    bbox_to_anchor=(0.5, -0.15), \n",
    "    ncol=5, \n",
    "    frameon=False, \n",
    "    prop={'size': 15}\n",
    ")\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "ax = plt.subplot(gs1[0, 2])\n",
    "ax.plot(x,Exact[75,:], 'b-', linewidth = 2, label = 'Exact')       \n",
    "ax.plot(x,U_pred[75,:], 'r--', linewidth = 2, label = 'Prediction')\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$u(t,x)$')\n",
    "ax.axis('square')\n",
    "ax.set_xlim([-1.1,1.1])\n",
    "ax.set_ylim([-1.1,1.1])    \n",
    "ax.set_title('$t = 0.75$', fontsize = 15)\n",
    "\n",
    "for item in ([ax.title, ax.xaxis.label, ax.yaxis.label] +\n",
    "             ax.get_xticklabels() + ax.get_yticklabels()):\n",
    "    item.set_fontsize(15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
